# combo_local_machine file updated with functions (object oriented programming)

# import yahoo finance data
import yfinance as yf
# import stockstats data
from stockstats import StockDataFrame as ss
# import necessary libraries
import matplotlib as mp
import numpy as np
import pandas as pd
import pytz
import warnings
import time
import random
import statistics
import pydoc
import os
import pyarrow
#import pandas_gbq
import statsmodels
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from sklearn.preprocessing import MinMaxScaler
import sqlite3
import pickle

#import libraries for SARIMA model
import pmdarima as pm
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import pickle as pkl
from datetime import datetime
from datetime import timedelta
from datetime import date
import matplotlib.pyplot as graph


''' 
1. AAPL - total yrs: 2015 - present
-- initial load: 2015-2023
-- appended load: 2023-present
--initial load -> run thru all functions (predicting w data only from 2015-2023)
--appended load -> run thru all functions

now: predicted vals in a df from historical data (2015-2023) -> save those values
     predicted vals in a df from historical data (2023-present)


     SARIMA MODEL:
     Input Ranges:
     1. 1-1-15 to 5-31-2024, 2 minutes 28 seconds
     2. 1-1-17 to 5-31-2024
     3. 1-1-19 to 5-31-2024
     4. 1-1-21 to 5-31-2024

     Predicting: 
     June 1st - June 11th, 2024

'''

def main():

    # 1: ----- SARIMA MODEL ----- (INITIAL LOAD)
    # ask user for stock ticker and print data
    stock_select_str = input("Enter stock ticker: ")
    # initial load run
    stock_data = yf.download(stock_select_str, start = '2024-04-01', end = '2024-05-31', interval = '1d')
    # scale data
    feature_scaler = MinMaxScaler()
    # run SARIMA functions for initial load
    initial_transformed, initial_model = load(stock_data, feature_scaler)
    print("initial load returned")
    # forecast initial load
    initial_df_pred_sarima = sarima_forecast(initial_model, initial_transformed, '2024-06-11', feature_scaler)
    print_metrics(initial_df_pred_sarima.tail(11), initial_model, stock_select_str)
    #print("IMPORTANT\n")
    #print(initial_df_pred_sarima.tail(11))
    #print(initial_df_pred_sarima)
    #plot = create_plot(initial_df_pred_sarima)
    #print(initial_df_pred_sarima["Actual"])
    #graph.plot(plot, label = "total predict")
    #graph.plot(actual, label = "total actual")
    graph.xlabel('Date')
    graph.ylabel('Stock Price')
    graph.legend()
    #graph.savefig('predicted_stock_prices_lstm_total.png')
    graph.savefig('test1.png')
    graph.show()

    '''
    # 2: ---- SARIMA MODEL ---- (APPENDED LOAD)
    appended_data = yf.download(stock_select_str, start = '2023-01-02', interval = '1d')
    # run SARIMA functions for appended load
    appended_transformed, appended_model = load(appended_data, feature_scaler)
    print("append load returned")
    # forecast appended load
    appended_df_pred_sarima = sarima_forecast(appended_model, appended_transformed, '2024-06-01', feature_scaler)
    print(appended_df_pred_sarima.tail())

    # 3: ---- SARIMA MODEL ---- FORECAST VALUES WEIGHT COMBINATION
    total = len(stock_data.index) + len(appended_data.index)
    print(total)
    weight_initial = float(len(stock_data.index) / total)
    print(weight_initial)
    '''


def load(stock_data, feature_scaler):
    # remove any duplicate index
    stock_data = stock_data.loc[~stock_data.index.duplicated(keep='first')]
    # filter only required data
    stock_data = stock_data[['Close']]
    # transform current stock data
    stock_transformed = pd.DataFrame(np.squeeze(feature_scaler.fit_transform(stock_data), axis=1), columns=["Close"], index=stock_data.index)
    # call function to fit a seasonal ARIMA model
    sarima_model = process_model(stock_transformed)
    # call function to serialize / deserialize SARIMA model with pickle
    sarima_model = serialize_deserialize(sarima_model)
    return stock_transformed, sarima_model

    
def process_model(stock_transformed):
    print("process model \n")
    sarima_model = pm.auto_arima(stock_transformed["Close"], start_p=1, start_q=1,
                         test='adf',
                         max_p=3, max_q=3,
                         m=12, #12 is the frequency of the cycle (yearly)
                         start_P=0,
                         seasonal=True, #set to seasonal
                         d=None,
                         D=1, #order of the seasonal differencing
                         trace=False,
                         error_action='ignore',
                         suppress_warnings=True,
                         stepwise=True)
    
    return sarima_model

def serialize_deserialize(sarima_model):
    print("serialize \n")
    # Serialize with Pickle and save it as pkl
    with open('sarima_model.pkl', 'wb') as pkl:
        pickle.dump(sarima_model, pkl)

    # Desiarilize the content of the file back into a Python object
    with open('sarima_model.pkl', 'rb') as pkl:
        loaded_model = pickle.load(pkl)

    return loaded_model

def sarima_forecast(model, df, forecast_date, Feature_Scaler):
    print("forecast \n")
    # Forecast
    n_periods = (datetime.strptime(forecast_date, '%Y-%m-%d') - df.index[-1]).days
    fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)
    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(days=1), periods = n_periods, freq='D')

    # Make series for plotting purpose
    fitted_series = pd.Series(fitted.values, index=index_of_fc)
    lower_series = pd.Series(confint[:, 0], index=index_of_fc)
    upper_series = pd.Series(confint[:, 1], index=index_of_fc)

    #Concatenate the original DataFrame with forecasted values and confidence intervals
    df_result = pd.concat([df, fitted_series, lower_series, upper_series], axis=1)
    df_result.columns = ["Actual", "Prediction", "Low", "High"]

    #Inverse transform the scaled values to their original scale
    for column in df_result.columns:
      df_result[column] = Feature_Scaler.inverse_transform(df_result[column].values.reshape(-1,1))

    return df_result

def print_metrics(df_result, model, stock_name):
    print("Calculating metrics...")
    # Print the columns in df_result for debugging
    print("Columns in df_result:", df_result.columns)

    stock_ticker = yf.Ticker(stock_name)
    #real_values = stock_ticker.history(start='2024-06-01', end='2024-06-11')
    real_values = yf.download(stock_name, end='2024-06-11')
    #print(real).tail(11))
    
    # Ensure that the DataFrame contains the required columns
    #required_columns = ['Actual', 'Prediction']
    #if not all(col in df_result.columns for col in required_columns):
       #raise ValueError(f"DataFrame must contain the columns: {required_columns}")

    # Calculate and print AIC and BIC
    aic = model.aic()
    bic = model.bic()
    print(f"AIC: {aic}")
    print(f"BIC: {bic}")

    # Calculate and print RMSE and MSE
    #df_result = df_result.dropna()
    print("BEOFRE\n")
    print(real_values.loc['2024-06-01':'2024-06-11', "Close"])
    print("SEC\n")

    #print("type1:", type(real_values.index))
    #print("type:", type(df_result.index))
    #print(df_result.loc['2024-06-01':'2024-06-11', "Prediction"])
    subtract = (real_values.loc['2024-06-01':'2024-06-11', "Close"] - df_result.loc['2024-06-01':'2024-06-11',"Prediction"])
    print("subtract\n")
    print(subtract)
    mse = (subtract ** 2).mean()
    print("real vals\n")
    print(real_values["Close"])
    print("predicted values\n")
    print(df_result["Prediction"])
    print(mse)
    rmse = np.sqrt(mse)
    print(f"MSE: {mse}")
    print(f"RMSE: {rmse}")

    # Calculate and print R-squared
    # actual values - mean of actual values
    ss_total = ((real_values.loc['2024-06-01':'2024-06-11', "Close"] - (real_values.loc['2024-06-01':'2024-06-11', "Close"]).mean()) ** 2).sum()
    print(ss_total)
    # residual sum of squares
    ss_residual = ((real_values.loc['2024-06-01':'2024-06-11', "Close"] - df_result["Prediction"]) ** 2).sum()
    print(ss_residual)

    r_squared = 1 - (ss_residual / ss_total)
    print(f"R-squared: {r_squared}")

    # Calculate p-value (This is approximate, more robust methods available)
    # Calculate p-value
    # Prepare data for OLS regression
    import statsmodels.api as sm
    X = sm.add_constant(df_result["Prediction"])
    y = real_values.loc['2024-06-01':'2024-06-11', "Close"]
    print(X)
    print(y)

    # Ensure indices match
    X = X.loc[y.index]
    y = y.loc[X.index]

    # Fit OLS model
    model_sm = sm.OLS(y, X).fit()
    
    # Extract and print p-value
    p_value = model_sm.pvalues[1]  # p-value for the 'Prediction' variable
    print(f"P-value: {p_value}")

'''

'''

'''
def create_plot(test):
    print("create plot \n")
    # Plot
    plt.figure(figsize=(15,7))
    plt.plot(test["Actual"][-120:], color='#1f76b4')
    plt.plot(test["Prediction"], color='darkgreen')
    plt.fill_between(test.index,
                test["Low"],
                test["High"],
                color='k', alpha=.15)

    plt.title("SARIMA - Forecast of AAPL Stock Price")
    plt.show()
'''



main()