{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Edits and Notes\n",
    "- 2024-01-30\n",
    "- meeting with Kari and Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what and why**\n",
    "## Description\n",
    "## Objective\n",
    "## Summary Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**how**\n",
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### packages and global stuff\n",
    "\n",
    "# import yahoo finance data\n",
    "import yfinance as yf\n",
    "# import stockstats data\n",
    "from stockstats import StockDataFrame as ss\n",
    "\n",
    "# import necessary libraries\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "import pydoc\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas_gbq\n",
    "import statsmodels\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_ticker = yf.Ticker(\"AAPL\")\n",
    "apple_data = yf.download(\"AAPL\", start = '2020-01-01', interval = '1d')\n",
    "apple_df = ss.retype(apple_data)\n",
    "\n",
    "apple_data[['stochrsi', 'macd', 'mfi']] = apple_df[['stochrsi', 'macd', 'mfi']]\n",
    "print(apple_data)\n",
    "print(apple_ticker.get_capital_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "# import google cloud service account and bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# specify google cloud project information\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'black-vehicle-406619-bf2e31773163.json')\n",
    "project_id = 'black-vehicle-406619'\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "dataset_id = 'stocks_ds'\n",
    "table_id = '20yrs_stockdata'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "load_data = client.load_table_from_dataframe(apple_data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "**should always be up front**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA functions\n",
    "\n",
    "def forecast(model, df, forecast_date):\n",
    "    # Forecast\n",
    "    n_periods = (datetime.strptime(forecast_date, '%Y-%m-%d') - df.index[-1]).days\n",
    "    fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(days=1), periods = n_periods, freq='D')\n",
    "\n",
    "    # Make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted.values, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    #Concatenate the original DataFrame with forecasted values and confidence intervals\n",
    "    df_result = pd.concat([df, fitted_series, lower_series, upper_series], axis=1)\n",
    "    df_result.columns = [\"Actual\", \"Prediction\", \"Low\", \"High\"]\n",
    "\n",
    "    #Inverse transform the scaled values to their original scale\n",
    "    for column in df_result.columns:\n",
    "      df_result[column] = Feature_Scaler.inverse_transform(df_result[column].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions\n",
    "\n",
    "def features_targets(data, feature_length):\n",
    "    # feature length is the number of time steps in the input sequence\n",
    "    # targets are the values the model is trying to forecast\n",
    "    time_step_list, close_label_list = [], []\n",
    "    \n",
    "    # iterate through (length of sequential data) to (length of seq data - feature length)\n",
    "    for i in range(len(data) - feature_length):\n",
    "        # this will get the vals leading up to the target\n",
    "        time_steps = data[i : i + feature_length]\n",
    "        time_step_list.append(time_steps)\n",
    "        # this will get the target val at this point\n",
    "        labels = data[i + feature_length]\n",
    "        close_label_list.append(labels)\n",
    "\n",
    "    # reshape lists to be suitable for network algo\n",
    "    time_step_list = np.array(time_step_list).reshape(len(time_step_list), feature_length, 1)\n",
    "    close_label_list = np.array(close_label_list).reshape(len(close_label_list), 1)\n",
    "\n",
    "    return time_step_list, close_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X, Y, df, data, train_test_slice, scaler):\n",
    "    # training set: set to train the machine learning model\n",
    "    # testing set: set used to test model after model has been trained\n",
    "    # train set is 70% of data, test set is the rest (30%)\n",
    "    X_train, X_test = X[:-train_test_slice], X[-train_test_slice:]\n",
    "    Y_train, Y_test = Y[:-train_test_slice], Y[-train_test_slice:]\n",
    "\n",
    "    # initialize empty model where nodes have input and output with Keras\n",
    "    model = Sequential()\n",
    "    # create a bidirectional LSTM: \n",
    "    # - 100 cells\n",
    "    # - return output for input\n",
    "    # - reduce overfitting w current dropout\n",
    "    # - specify # of steps for target\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.1, input_shape=(X_train.shape[1], 1))))\n",
    "    # provide additional processing with undirectional layer\n",
    "    model.add(LSTM(50, recurrent_dropout=0.1))\n",
    "    # add dropout and dense layers\n",
    "    # randomly sets 20% of inputs to 0 to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 25 output units\n",
    "    model.add(Dense(20, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 10 output units\n",
    "    model.add(Dense(10))\n",
    "    # create a connected layer with 1 output unit\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # optimize model using stochastic gradient descent to train model\n",
    "    # SGD \n",
    "    optimize = tf.keras.optimizers.SGD(learning_rate = 0.002)\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimize)\n",
    "    # save model weights validation loss improves\n",
    "    weights = ModelCheckpoint(\"best_weights.h5\", monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    # adjust learning rate when needed\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # one epoch completes when the entire training dataset is processed once by the model\n",
    "    model.fit(X_train, Y_train, epochs=12, batch_size=1, verbose=1, shuffle=False, validation_data=(X_test, Y_test), callbacks=[reduce_lr, weights])\n",
    "    actual = scaler.inverse_transform(Y_test)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    actual = np.squeeze(actual, axis=1)\n",
    "    predictions = np.squeeze(predictions, axis=1)\n",
    "\n",
    "    #reassign index before it goes into model\n",
    "    test_df = pd.DataFrame({'Actual': actual, 'Predicted': predictions.flatten()})\n",
    "    print(test_df)\n",
    "    #return model, loss, predict, predict_inv, Y_test, scaler\n",
    "\n",
    "    '''\n",
    "    # Plotting test set\n",
    "    graph.plot(df.index[-train_test_slice:], predictions, label=\"Predicted\")\n",
    "    graph.plot(df.index[-train_test_slice:], actual, label=\"Actual\")\n",
    "    graph.xlabel('Date')\n",
    "    graph.ylabel('Stock Price')\n",
    "    graph.legend()\n",
    "    graph.savefig('predicted_stock_prices_lstm3_test.png')\n",
    "    graph.show()\n",
    "    '''\n",
    "    return model, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(model, df, future_date, scaler, feature_length=20):\n",
    "    # iterate through today's date until future date\n",
    "    for i in range((datetime.strptime(future_date, '%Y-%m-%d') - df.index[-1]).days):\n",
    "        # specify close values\n",
    "        feature_column = df['Close'].values\n",
    "        # pick out last 20 days\n",
    "        time_steps = feature_column[-feature_length:]\n",
    "        # reshape array\n",
    "        time_steps = time_steps.reshape(feature_length, 1)\n",
    "        # scale array\n",
    "        time_steps = scaler.transform(time_steps)\n",
    "        prediction = model.predict(time_steps.reshape(1, feature_length, 1))\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "        # concatenate results with og dataframe\n",
    "        df_forecast = pd.DataFrame(prediction, index=[df.index[-1] + timedelta(days=1)], columns=['Close'])\n",
    "        df = pd.concat([df, df_forecast])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SARIMA\n",
    "**why?**  \n",
    "\n",
    "SARIMA MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import libraries for SARIMA model\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal - fit stepwise auto-ARIMA\n",
    "#!pip install pmdarima\n",
    "\n",
    "# Remove any duplicate index\n",
    "apple_data = apple_data.loc[~apple_data.index.duplicated(keep='first')]\n",
    "\n",
    "#Filter only required data\n",
    "\n",
    "apple_data = apple_data[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the APPL data into a standard range using MinMaxScaler ()\n",
    "Feature_Scaler = MinMaxScaler()\n",
    "\n",
    "#Transform current APPL data\n",
    "apple_transformed = pd.DataFrame(np.squeeze(Feature_Scaler.fit_transform(apple_data), axis=1), columns=[\"Close\"], index=apple_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarima_model = pm.auto_arima(apple_transformed[\"Close\"], start_p=1, start_q=1,\n",
    "                         test='adf',\n",
    "                         max_p=3, max_q=3,\n",
    "                         m=12, #12 is the frequency of the cycle (yearly)\n",
    "                         start_P=0,\n",
    "                         seasonal=True, #set to seasonal\n",
    "                         d=None,\n",
    "                         D=1, #order of the seasonal differencing\n",
    "                         trace=False,\n",
    "                         error_action='ignore',\n",
    "                         suppress_warnings=True,\n",
    "                         stepwise=True)\n",
    "\n",
    "# start_p=1, start_q=1: Sets the initial values for the order of the AR (AutoRegressive) and MA (Moving Average) components in the non-seasonal part of the model.\n",
    "# test='adf': Specifies the use of the Augmented Dickey-Fuller (ADF) test to determine whether the time series is stationary and to help in determining the need for differencing (`d` parameter).\n",
    "# max_p=3, max_q=3: Specifies the maximum values for the `p` and `q` parameters to consider during the model fitting process.\n",
    "# start_P=0: Sets the initial value for the order of the seasonal AR component.\n",
    "# d=None: The order of non-seasonal differencing is not specified, which allows the function to determine it automatically.\n",
    "# trace=False: This means that the function will not print out diagnostic information about the steps it's taking.\n",
    "# error_action='ignore': Instructs the function to ignore errors and try different combinations of parameters.\n",
    "# suppress_warnings=True: Suppresses convergence warnings, which can be frequent in ARIMA modeling.\n",
    "# stepwise=True: Enables a stepwise search to efficiently find the best model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA model plot diagnostics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sarima_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wjulia800/stock_forecasting/combo.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sarima_model\u001b[39m.\u001b[39mplot_diagnostics(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m,\u001b[39m12\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sarima_model' is not defined"
     ]
    }
   ],
   "source": [
    "sarima_model.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize with Pickle and save it as pkl\n",
    "with open('sarima_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(sarima_model, pkl)\n",
    "\n",
    "# Desiarilize the content of the file back into a Python object\n",
    "with open('sarima_model.pkl', 'rb') as pkl:\n",
    "    loaded_model = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=forecast(loaded_model, apple_transformed, \"2023-12-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(test[\"Actual\"][-120:], color='#1f76b4')\n",
    "plt.plot(test[\"Prediction\"], color='darkgreen')\n",
    "plt.fill_between(test.index,\n",
    "                test[\"Low\"],\n",
    "                test[\"High\"],\n",
    "                color='k', alpha=.15)\n",
    "\n",
    "plt.title(\"SARIMA - Forecast of APPL Stock Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LSTM\n",
    "**why?**  \n",
    "\n",
    "LSTM MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple_data = yf.download(\"AAPL\", start = \"2020-01-01\", interval = '1d')\n",
    "apple_df_test = apple_data.reset_index()\n",
    "apple_df = apple_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_transformed = pd.DataFrame(\n",
    "    np.squeeze(\n",
    "        scaler.fit_transform(\n",
    "            apple_df[[\"Close\"]])), columns=[\"Close\"], index=apple_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_vals, target_vals = features_targets(data_transformed[\"Close\"].values, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals_cutoff = apple_df.loc[apple_df['Date'] >= '2022-01-01']\n",
    "slice = train_vals_cutoff.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X_train, X_test, Y_train, Y_test = create_model(\n",
    "        time_step_vals, target_vals, apple_df, data_transformed[\"Close\"].values, slice, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x = np.concatenate((X_train, X_test), axis = 0)\n",
    "total_y = np.concatenate((Y_train, Y_test), axis = 0)\n",
    "final_predict = model.predict(total_x)\n",
    "final_predict = scaler.inverse_transform(final_predict)\n",
    "actual = scaler.inverse_transform(total_y)\n",
    "final_predict = np.squeeze(final_predict, axis = 1)\n",
    "actual = np.squeeze(actual, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot(final_predict, label = \"total predict\")\n",
    "graph.plot(actual, label = \"total actual\")\n",
    "graph.xlabel('Date')\n",
    "graph.ylabel('Stock Price')\n",
    "graph.legend()\n",
    "graph.savefig('test.png')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_transformed[-21:-1].values.reshape(1,20,1))\n",
    "data = pd.DataFrame(apple_data)\n",
    "test = predict_lstm(model, data, '2024-02-01', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: is this where the combo assessment goes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Why does SARIMA only have 1 function, but LSTM has several?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify google cloud project information\n",
    "dataset_id = 'predicted_prices'\n",
    "table_id = 'SARIMA and LTSM Predicted Prices'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "# enter combined data load_data = client.load_table_from_dataframe(combined data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "** a bit more detail about why a client would need this and what value it would bring to their work **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
