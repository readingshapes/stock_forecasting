{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Edits and Notes\n",
    "- 2024-01-30\n",
    "- meeting with Kari and Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*what and why*\n",
    "## Description\n",
    "The United States Stock Market faces fluctuations due to supply, demand, current events, etc. There are many variables that cause the stock market to experience random behavior. However, analyzing trends in the market from previous years and applying forecasting methods can allow systems with random behaviors to be predicted with high accuracy. In this case, the stock market is a system with random behavior. By using Long Short-Term Memory (LSTM) models (a Deep Learning network), and SARIMA (Seasonal Auto-Regressive Integrated Moving Average) models, stock prices can be predicted with high accuracy.\n",
    "\n",
    "## Objective\n",
    "The objective of this project is to predict stock market prices with Machine Learning methods. For this project, the Apple Stock (AAPL) will be predicted for the date 03-01-2024. Twenty years of historical data will be used as reference from Yahoo Finance. The main objective will be to create SARIMA and LSTM models; and eventually combine them to have a final prediction based on both models.\n",
    "\n",
    "## Summary Findings\n",
    "\n",
    "Predicted values for 03-01-2024:\n",
    "\n",
    "SARIMA Model: <br> \n",
    "\n",
    "Date    Prediction\tLow\t        High <br> \n",
    "2024-02-26\t184.833321\t159.771071\t209.895571 <br> \n",
    "2024-02-27\t188.434224\t162.823625\t214.044823 <br> \n",
    "2024-02-28\t189.007441\t162.902321\t215.112561 <br> \n",
    "2024-02-29\t190.148931\t163.596792\t216.701069 <br> \n",
    "2024-03-01\t190.322416\t163.365394\t217.279439 <br> \n",
    "\n",
    "LSTM Model: <br> \n",
    "\n",
    "Date\t    Prediction <br> \n",
    "2024-02-26\t176.886795\tNaN\tNaN <br> \n",
    "2024-02-27\t176.718079\tNaN\tNaN <br> \n",
    "2024-02-28\t176.557510\tNaN\tNaN <br> \n",
    "2024-02-29\t176.404694\tNaN\tNaN <br> \n",
    "2024-03-01\t176.259399\tNaN\tNaN <br> \n",
    "\n",
    "The SARIMA Model predicted the cost of the AAPL stock to be $190.322 on 03-01-2024; whereas the LSTM Model Predicted the AAPL Stock to be $176.259 on 03-01-2024. For the time period of the last week of February and the first week of March 2024, the SARIMA Model predicts the stock to cost around $184 - $190; the LSTM Model predicts the stock to cost around $170 - $180. \n",
    "\n",
    "The SARIMA Model captures the short-/medium-term depedencies in data and the LSTM Model captures long-term dependencies in data. The SARIMA Model is more surface-level compared to the LSTM Model, because the SARIMA Model forecasts based on apparent seasonal patterns and periodic variations. However, the LSTM Model forecasts in a more complicated manner, as it takes in more variables in order to learn long-term dependencies. \n",
    "\n",
    "From the results, it can be inferred the SARIMA Model found general seasonal trends that caused it to forecast a higher range of cost; and the LSTM Model found long-term dependencies that caused it to place emphasis on a smaller range of cost for the intended time period. This shows how it is beneficial to combine both models, because the combination of both models can allow all short-term, medium-term, and long-term dependencies to be factors within the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*how*\n",
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yfinance\n",
    "# %pip install stockstats\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install pytz\n",
    "# %pip install statistics\n",
    "# %pip install os\n",
    "# %pip install pyarrow\n",
    "# %pip install pandas_gbq\n",
    "# %pip install statsmodels\n",
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 20:40:19.194698: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-02 20:40:20.030533: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 20:40:20.030639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 20:40:20.198549: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 20:40:20.544955: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-02 20:40:20.550138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 20:40:23.164455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### packages and global stuff\n",
    "\n",
    "# import yahoo finance data\n",
    "import yfinance as yf\n",
    "# import stockstats data\n",
    "from stockstats import StockDataFrame as ss\n",
    "\n",
    "# import necessary libraries\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "import pydoc\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas_gbq\n",
    "import statsmodels\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "                  open        high         low       close   adj close  \\\n",
      "Date                                                                     \n",
      "2020-01-02   74.059998   75.150002   73.797501   75.087502   73.152657   \n",
      "2020-01-03   74.287498   75.144997   74.125000   74.357498   72.441467   \n",
      "2020-01-06   73.447502   74.989998   73.187500   74.949997   73.018684   \n",
      "2020-01-07   74.959999   75.224998   74.370003   74.597504   72.675285   \n",
      "2020-01-08   74.290001   76.110001   74.290001   75.797501   73.844345   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2024-01-26  194.270004  194.759995  191.940002  192.419998  192.419998   \n",
      "2024-01-29  192.009995  192.199997  189.580002  191.729996  191.729996   \n",
      "2024-01-30  190.940002  191.800003  187.470001  188.039993  188.039993   \n",
      "2024-01-31  187.039993  187.100006  184.350006  184.399994  184.399994   \n",
      "2024-02-01  183.990005  186.949997  183.820007  186.860001  186.860001   \n",
      "\n",
      "               volume    stochrsi      macd       mfi  \n",
      "Date                                                   \n",
      "2020-01-02  135480400         NaN  0.000000  0.500000  \n",
      "2020-01-03  146322800         NaN -0.016378  0.500000  \n",
      "2020-01-06  118387200  100.000000 -0.002496  0.500000  \n",
      "2020-01-07  108872000   76.992988 -0.008847  0.500000  \n",
      "2020-01-08  132079200  100.000000  0.035638  0.500000  \n",
      "...               ...         ...       ...       ...  \n",
      "2024-01-26   44594000   72.999542  0.951170  0.655291  \n",
      "2024-01-29   47145600   66.461205  0.943601  0.586400  \n",
      "2024-01-30   55859400   35.400724  0.632559  0.520201  \n",
      "2024-01-31   55467800   11.169631  0.091286  0.454385  \n",
      "2024-02-01   64885400   32.281798 -0.137589  0.527080  \n",
      "\n",
      "[1028 rows x 9 columns]\n",
      "<bound method TickerBase.get_capital_gains of yfinance.Ticker object <AAPL>>\n"
     ]
    }
   ],
   "source": [
    "apple_ticker = yf.Ticker(\"AAPL\")\n",
    "apple_data = yf.download(\"AAPL\", start = '2020-01-01', interval = '1d')\n",
    "apple_df = ss.retype(apple_data)\n",
    "\n",
    "apple_data[['stochrsi', 'macd', 'mfi']] = apple_df[['stochrsi', 'macd', 'mfi']]\n",
    "print(apple_data)\n",
    "print(apple_ticker.get_capital_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=black-vehicle-406619, location=US, id=be91c362-d06d-4698-8a1d-8ca099a71bc6>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "# import google cloud service account and bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# specify google cloud project information\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'black-vehicle-406619-bf2e31773163.json')\n",
    "project_id = 'black-vehicle-406619'\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "dataset_id = 'stocks_ds'\n",
    "table_id = '20yrs_stockdata'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "load_data = client.load_table_from_dataframe(apple_data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "*should always be up front*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA functions\n",
    "\n",
    "def forecast_sarima(model, df, forecast_date):\n",
    "    # Forecast\n",
    "    n_periods = (datetime.strptime(forecast_date, '%Y-%m-%d') - df.index[-1]).days\n",
    "    fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(days=1), periods = n_periods, freq='D')\n",
    "\n",
    "    # Make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted.values, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    #Concatenate the original DataFrame with forecasted values and confidence intervals\n",
    "    df_result = pd.concat([df, fitted_series, lower_series, upper_series], axis=1)\n",
    "    df_result.columns = [\"Actual\", \"Prediction\", \"Low\", \"High\"]\n",
    "\n",
    "    #Inverse transform the scaled values to their original scale\n",
    "    for column in df_result.columns:\n",
    "      df_result[column] = Feature_Scaler.inverse_transform(df_result[column].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions\n",
    "\n",
    "def features_targets_lstm(data, feature_length):\n",
    "    # feature length is the number of time steps in the input sequence\n",
    "    # targets are the values the model is trying to forecast\n",
    "    time_step_list, close_label_list = [], []\n",
    "    \n",
    "    # iterate through (length of sequential data) to (length of seq data - feature length)\n",
    "    for i in range(len(data) - feature_length):\n",
    "        # this will get the vals leading up to the target\n",
    "        time_steps = data[i : i + feature_length]\n",
    "        time_step_list.append(time_steps)\n",
    "        # this will get the target val at this point\n",
    "        labels = data[i + feature_length]\n",
    "        close_label_list.append(labels)\n",
    "\n",
    "    # reshape lists to be suitable for network algo\n",
    "    time_step_list = np.array(time_step_list).reshape(len(time_step_list), feature_length, 1)\n",
    "    close_label_list = np.array(close_label_list).reshape(len(close_label_list), 1)\n",
    "\n",
    "    return time_step_list, close_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm(X, Y, df, data, train_test_slice, scaler):\n",
    "    # training set: set to train the machine learning model\n",
    "    # testing set: set used to test model after model has been trained\n",
    "    # train set is 70% of data, test set is the rest (30%)\n",
    "    X_train, X_test = X[:-train_test_slice], X[-train_test_slice:]\n",
    "    Y_train, Y_test = Y[:-train_test_slice], Y[-train_test_slice:]\n",
    "\n",
    "    # initialize empty model where nodes have input and output with Keras\n",
    "    model = Sequential()\n",
    "    # create a bidirectional LSTM: \n",
    "    # - 100 cells\n",
    "    # - return output for input\n",
    "    # - reduce overfitting w current dropout\n",
    "    # - specify # of steps for target\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.1, input_shape=(X_train.shape[1], 1))))\n",
    "    # provide additional processing with undirectional layer\n",
    "    model.add(LSTM(50, recurrent_dropout=0.1))\n",
    "    # add dropout and dense layers\n",
    "    # randomly sets 20% of inputs to 0 to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 25 output units\n",
    "    model.add(Dense(20, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 10 output units\n",
    "    model.add(Dense(10))\n",
    "    # create a connected layer with 1 output unit\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # optimize model using stochastic gradient descent to train model\n",
    "    # SGD \n",
    "    optimize = tf.keras.optimizers.SGD(learning_rate = 0.002)\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimize)\n",
    "    # save model weights validation loss improves\n",
    "    weights = ModelCheckpoint(\"best_weights.h5\", monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    # adjust learning rate when needed\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # one epoch completes when the entire training dataset is processed once by the model\n",
    "    model.fit(X_train, Y_train, epochs=12, batch_size=1, verbose=1, shuffle=False, validation_data=(X_test, Y_test), callbacks=[reduce_lr, weights])\n",
    "    actual = scaler.inverse_transform(Y_test)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    actual = np.squeeze(actual, axis=1)\n",
    "    predictions = np.squeeze(predictions, axis=1)\n",
    "\n",
    "    #reassign index before it goes into model\n",
    "    test_df = pd.DataFrame({'Actual': actual, 'Predicted': predictions.flatten()})\n",
    "    print(test_df)\n",
    "    #return model, loss, predict, predict_inv, Y_test, scaler\n",
    "\n",
    "    return model, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(model, df, future_date, scaler, feature_length=20):\n",
    "    # iterate through today's date until future date\n",
    "    for i in range((datetime.strptime(future_date, '%Y-%m-%d') - df.index[-1]).days):\n",
    "        # specify close values\n",
    "        feature_column = df['Close'].values\n",
    "        # pick out last 20 days\n",
    "        time_steps = feature_column[-feature_length:]\n",
    "        # reshape array\n",
    "        time_steps = time_steps.reshape(feature_length, 1)\n",
    "        # scale array\n",
    "        time_steps = scaler.transform(time_steps)\n",
    "        prediction = model.predict(time_steps.reshape(1, feature_length, 1))\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "        # concatenate results with og dataframe\n",
    "        df_forecast = pd.DataFrame(prediction, index=[df.index[-1] + timedelta(days=1)], columns=['Close'])\n",
    "        df = pd.concat([df, df_forecast])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combined_models(data_transformed, lstm_model, sarima_model, forecast_date, scaler):\n",
    "\n",
    "  # create dataframes for high and low values\n",
    "  df_high = pd.DataFrame(columns=['High'])\n",
    "  df_low = pd.DataFrame(columns=['Low'])\n",
    "  #last_initial_date = data_transformed.index[-1]\n",
    "  # create variable referring to last day\n",
    "  last_day = data_transformed.index[-1]\n",
    "  \n",
    "    # iterate through the number of days between last day and future date\n",
    "  for i in range((datetime.strptime(forecast_date, '%Y-%m-%d') - data_transformed.index[-1]).days):\n",
    "    # call prediction function for lstm\n",
    "    df_lstm_temp = predict_lstm(lstm_model, data_transformed, (data_transformed.index[-1] + timedelta(days=1)).strftime('%Y-%m-%d'), scaler)\n",
    "    # call forecast function for sarima\n",
    "    df_sarima_temp = forecast_sarima(sarima_model, data_transformed, (data_transformed.index[-1] + timedelta(days=i+1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "    # combine 30% of lstm results and 70% of sarima results for closing vals on last day (day to predict)\n",
    "    combination = 0.3 * (scaler.transform(df_lstm_temp.iloc[-1]['Close'].reshape(-1,1))) + 0.7 * (scaler.transform(df_sarima_temp.iloc[-1]['Prediction'].reshape(-1,1)))\n",
    "    print(combination)\n",
    "\n",
    "    # create new dataframe of combination\n",
    "    forecast = pd.DataFrame(combination, index = [data_transformed.index[-1] + timedelta(days=1)], columns = ['Close'])\n",
    "    data_transformed = pd.concat([data_transformed, forecast])\n",
    "\n",
    "    # create new dataframe of low values\n",
    "    df_low = pd.concat([df_low, pd.DataFrame(df_sarima_temp.iloc[-1]['Low'],\n",
    "                                          index=[data_transformed.index[-1]+ timedelta(days=1)],\n",
    "                                          columns=['Low'])\n",
    "    ])\n",
    "    # create new dataframe of high values\n",
    "    df_high = pd.concat([df_high, pd.DataFrame(df_sarima_temp.iloc[-1]['High'],\n",
    "                                          index=[data_transformed.index[-1]+ timedelta(days=1)],\n",
    "                                          columns=['High'])\n",
    "    ])\n",
    "\n",
    "  # squeeze final results to one axis, and inverse transform\n",
    "  df_final = pd.DataFrame(np.squeeze(scaler.inverse_transform(data_transformed)),\n",
    "                        index=data_transformed.index, columns=['Close'])\n",
    "\n",
    "  # final actual df values are all days leading up to last day that has occurred\n",
    "  df_final_actual = df_final[:last_day + timedelta(days=1)]\n",
    "  # final predicted df values are all future days\n",
    "  df_final_prediction = df_final[last_day + timedelta(days=1):]\n",
    "  print(df_final_prediction.tail())\n",
    "\n",
    "  return df_final_actual, df_final_prediction, df_low,  df_high\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SARIMA\n",
    "*why?*  \n",
    "\n",
    "SARIMA MODEL:\n",
    "The Seasonal Autoregressive Integrated Moving Average (SARIMA) Model is commonly used for stock forecasting. \n",
    "\n",
    "Seasonality: The SARIMA model is useful for this application because it works well with data exhibiting seasonal patterns. \n",
    "Autoregressive: The SARIMA model finds dependencies between current and historical data.\n",
    "Integrated: \n",
    "Moving Average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for SARIMA model\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal - fit stepwise auto-ARIMA\n",
    "#!pip install pmdarima\n",
    "\n",
    "# Remove any duplicate index\n",
    "apple_data = apple_data.loc[~apple_data.index.duplicated(keep='first')]\n",
    "\n",
    "#Filter only required data\n",
    "\n",
    "apple_data = apple_data[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the APPL data into a standard range using MinMaxScaler ()\n",
    "Feature_Scaler = MinMaxScaler()\n",
    "\n",
    "#Transform current APPL data\n",
    "apple_transformed = pd.DataFrame(np.squeeze(Feature_Scaler.fit_transform(apple_data), axis=1), columns=[\"Close\"], index=apple_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarima_model = pm.auto_arima(apple_transformed[\"Close\"], start_p=1, start_q=1,\n",
    "                         test='adf',\n",
    "                         max_p=3, max_q=3,\n",
    "                         m=12, #12 is the frequency of the cycle (yearly)\n",
    "                         start_P=0,\n",
    "                         seasonal=True, #set to seasonal\n",
    "                         d=None,\n",
    "                         D=1, #order of the seasonal differencing\n",
    "                         trace=False,\n",
    "                         error_action='ignore',\n",
    "                         suppress_warnings=True,\n",
    "                         stepwise=True)\n",
    "\n",
    "# start_p=1, start_q=1: Sets the initial values for the order of the AR (AutoRegressive) and MA (Moving Average) components in the non-seasonal part of the model.\n",
    "# test='adf': Specifies the use of the Augmented Dickey-Fuller (ADF) test to determine whether the time series is stationary and to help in determining the need for differencing (`d` parameter).\n",
    "# max_p=3, max_q=3: Specifies the maximum values for the `p` and `q` parameters to consider during the model fitting process.\n",
    "# start_P=0: Sets the initial value for the order of the seasonal AR component.\n",
    "# d=None: The order of non-seasonal differencing is not specified, which allows the function to determine it automatically.\n",
    "# trace=False: This means that the function will not print out diagnostic information about the steps it's taking.\n",
    "# error_action='ignore': Instructs the function to ignore errors and try different combinations of parameters.\n",
    "# suppress_warnings=True: Suppresses convergence warnings, which can be frequent in ARIMA modeling.\n",
    "# stepwise=True: Enables a stepwise search to efficiently find the best model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA model plot diagnostics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize with Pickle and save it as pkl\n",
    "with open('sarima_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(sarima_model, pkl)\n",
    "\n",
    "# Desiarilize the content of the file back into a Python object\n",
    "with open('sarima_model.pkl', 'rb') as pkl:\n",
    "    loaded_model = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=forecast(loaded_model, apple_transformed, \"2023-12-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wjulia800/stock_forecasting/Suggested_edits_combo.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/Suggested_edits_combo.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/Suggested_edits_combo.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m,\u001b[39m7\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/Suggested_edits_combo.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(test[\u001b[39m\"\u001b[39m\u001b[39mActual\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m120\u001b[39m:], color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m#1f76b4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/Suggested_edits_combo.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(test[\u001b[39m\"\u001b[39m\u001b[39mPrediction\u001b[39m\u001b[39m\"\u001b[39m], color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdarkgreen\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(test[\"Actual\"][-120:], color='#1f76b4')\n",
    "plt.plot(test[\"Prediction\"], color='darkgreen')\n",
    "plt.fill_between(test.index,\n",
    "                test[\"Low\"],\n",
    "                test[\"High\"],\n",
    "                color='k', alpha=.15)\n",
    "\n",
    "plt.title(\"SARIMA - Forecast of APPL Stock Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LSTM\n",
    "*why?*  \n",
    "\n",
    "LSTM MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple_data = yf.download(\"AAPL\", start = \"2020-01-01\", interval = '1d')\n",
    "apple_df_test = apple_data.reset_index()\n",
    "apple_df = apple_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_transformed = pd.DataFrame(\n",
    "    np.squeeze(\n",
    "        scaler.fit_transform(\n",
    "            apple_df[[\"Close\"]])), columns=[\"Close\"], index=apple_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_vals, target_vals = features_targets(data_transformed[\"Close\"].values, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals_cutoff = apple_df.loc[apple_df['Date'] >= '2022-01-01']\n",
    "slice = train_vals_cutoff.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X_train, X_test, Y_train, Y_test = create_model(\n",
    "        time_step_vals, target_vals, apple_df, data_transformed[\"Close\"].values, slice, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x = np.concatenate((X_train, X_test), axis = 0)\n",
    "total_y = np.concatenate((Y_train, Y_test), axis = 0)\n",
    "final_predict = model.predict(total_x)\n",
    "final_predict = scaler.inverse_transform(final_predict)\n",
    "actual = scaler.inverse_transform(total_y)\n",
    "final_predict = np.squeeze(final_predict, axis = 1)\n",
    "actual = np.squeeze(actual, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot(final_predict, label = \"total predict\")\n",
    "graph.plot(actual, label = \"total actual\")\n",
    "graph.xlabel('Date')\n",
    "graph.ylabel('Stock Price')\n",
    "graph.legend()\n",
    "graph.savefig('test.png')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_transformed[-21:-1].values.reshape(1,20,1))\n",
    "data = pd.DataFrame(apple_data)\n",
    "test = predict_lstm(model, data, '2024-02-01', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: is this where the combo assessment goes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Why does SARIMA only have 1 function, but LSTM has several?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: do you want to output plots as pngs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify google cloud project information\n",
    "dataset_id = 'predicted_prices'\n",
    "table_id = 'SARIMA and LTSM Predicted Prices'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "# enter combined data load_data = client.load_table_from_dataframe(combined data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "** a bit more detail about why a client would need this and what value it would bring to their work **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
