{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Edits and Notes\n",
    "- 2024-01-30\n",
    "- meeting with Kari and Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*what and why*\n",
    "## Description\n",
    "## Objective\n",
    "## Summary Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*how*\n",
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yfinance\n",
    "# %pip install stockstats\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install pytz\n",
    "# %pip install statistics\n",
    "# %pip install os\n",
    "# %pip install pyarrow\n",
    "# %pip install pandas_gbq\n",
    "# %pip install statsmodels\n",
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 21:31:09.733096: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 21:31:10.444235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 21:31:10.444314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 21:31:10.602832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 21:31:10.923552: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 21:31:10.927493: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 21:31:13.149479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### packages and global stuff\n",
    "\n",
    "# import yahoo finance data\n",
    "import yfinance as yf\n",
    "# import stockstats data\n",
    "from stockstats import StockDataFrame as ss\n",
    "\n",
    "# import necessary libraries\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "import pydoc\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas_gbq\n",
    "import statsmodels\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kari/.local/lib/python3.9/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  open        high         low       close   adj close  \\\n",
      "Date                                                                     \n",
      "2020-01-02   74.059998   75.150002   73.797501   75.087502   73.152634   \n",
      "2020-01-03   74.287498   75.144997   74.125000   74.357498   72.441444   \n",
      "2020-01-06   73.447502   74.989998   73.187500   74.949997   73.018707   \n",
      "2020-01-07   74.959999   75.224998   74.370003   74.597504   72.675278   \n",
      "2020-01-08   74.290001   76.110001   74.290001   75.797501   73.844360   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2024-01-24  195.419998  196.380005  194.339996  194.500000  194.500000   \n",
      "2024-01-25  195.220001  196.270004  193.110001  194.169998  194.169998   \n",
      "2024-01-26  194.270004  194.759995  191.940002  192.419998  192.419998   \n",
      "2024-01-29  192.009995  192.199997  189.580002  191.729996  191.729996   \n",
      "2024-01-30  190.940002  191.800003  187.470001  188.039993  188.039993   \n",
      "\n",
      "               volume    stochrsi      macd       mfi  \n",
      "Date                                                   \n",
      "2020-01-02  135480400         NaN  0.000000  0.500000  \n",
      "2020-01-03  146322800         NaN -0.016378  0.500000  \n",
      "2020-01-06  118387200  100.000000 -0.002496  0.500000  \n",
      "2020-01-07  108872000   76.992988 -0.008847  0.500000  \n",
      "2020-01-08  132079200  100.000000  0.035638  0.500000  \n",
      "...               ...         ...       ...       ...  \n",
      "2024-01-24   53631300   94.974494  0.590807  0.632481  \n",
      "2024-01-25   54822100   92.455043  0.876741  0.643113  \n",
      "2024-01-26   44553400   72.999542  0.951170  0.655327  \n",
      "2024-01-29   47145600   66.461205  0.943601  0.586432  \n",
      "2024-01-30   55186221   35.400724  0.632559  0.520693  \n",
      "\n",
      "[1026 rows x 9 columns]\n",
      "<bound method TickerBase.get_capital_gains of yfinance.Ticker object <AAPL>>\n"
     ]
    }
   ],
   "source": [
    "apple_ticker = yf.Ticker(\"AAPL\")\n",
    "apple_data = yf.download(\"AAPL\", start = '2020-01-01', interval = '1d')\n",
    "apple_df = ss.retype(apple_data)\n",
    "\n",
    "apple_data[['stochrsi', 'macd', 'mfi']] = apple_df[['stochrsi', 'macd', 'mfi']]\n",
    "print(apple_data)\n",
    "print(apple_ticker.get_capital_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=black-vehicle-406619, location=US, id=7cc9ca51-5775-4064-9a97-a208496af94c>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "# import google cloud service account and bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# specify google cloud project information\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'black-vehicle-406619-bf2e31773163.json')\n",
    "project_id = 'black-vehicle-406619'\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "dataset_id = 'stocks_ds'\n",
    "table_id = '20yrs_stockdata'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "load_data = client.load_table_from_dataframe(apple_data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "*should always be up front*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA functions\n",
    "\n",
    "def forecast(model, df, forecast_date):\n",
    "    # Forecast\n",
    "    n_periods = (datetime.strptime(forecast_date, '%Y-%m-%d') - df.index[-1]).days\n",
    "    fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(days=1), periods = n_periods, freq='D')\n",
    "\n",
    "    # Make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted.values, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    #Concatenate the original DataFrame with forecasted values and confidence intervals\n",
    "    df_result = pd.concat([df, fitted_series, lower_series, upper_series], axis=1)\n",
    "    df_result.columns = [\"Actual\", \"Prediction\", \"Low\", \"High\"]\n",
    "\n",
    "    #Inverse transform the scaled values to their original scale\n",
    "    for column in df_result.columns:\n",
    "      df_result[column] = Feature_Scaler.inverse_transform(df_result[column].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions\n",
    "\n",
    "def features_targets(data, feature_length):\n",
    "    # feature length is the number of time steps in the input sequence\n",
    "    # targets are the values the model is trying to forecast\n",
    "    time_step_list, close_label_list = [], []\n",
    "    \n",
    "    # iterate through (length of sequential data) to (length of seq data - feature length)\n",
    "    for i in range(len(data) - feature_length):\n",
    "        # this will get the vals leading up to the target\n",
    "        time_steps = data[i : i + feature_length]\n",
    "        time_step_list.append(time_steps)\n",
    "        # this will get the target val at this point\n",
    "        labels = data[i + feature_length]\n",
    "        close_label_list.append(labels)\n",
    "\n",
    "    # reshape lists to be suitable for network algo\n",
    "    time_step_list = np.array(time_step_list).reshape(len(time_step_list), feature_length, 1)\n",
    "    close_label_list = np.array(close_label_list).reshape(len(close_label_list), 1)\n",
    "\n",
    "    return time_step_list, close_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X, Y, df, data, train_test_slice, scaler):\n",
    "    # training set: set to train the machine learning model\n",
    "    # testing set: set used to test model after model has been trained\n",
    "    # train set is 70% of data, test set is the rest (30%)\n",
    "    X_train, X_test = X[:-train_test_slice], X[-train_test_slice:]\n",
    "    Y_train, Y_test = Y[:-train_test_slice], Y[-train_test_slice:]\n",
    "\n",
    "    # initialize empty model where nodes have input and output with Keras\n",
    "    model = Sequential()\n",
    "    # create a bidirectional LSTM: \n",
    "    # - 100 cells\n",
    "    # - return output for input\n",
    "    # - reduce overfitting w current dropout\n",
    "    # - specify # of steps for target\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.1, input_shape=(X_train.shape[1], 1))))\n",
    "    # provide additional processing with undirectional layer\n",
    "    model.add(LSTM(50, recurrent_dropout=0.1))\n",
    "    # add dropout and dense layers\n",
    "    # randomly sets 20% of inputs to 0 to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 25 output units\n",
    "    model.add(Dense(20, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 10 output units\n",
    "    model.add(Dense(10))\n",
    "    # create a connected layer with 1 output unit\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # optimize model using stochastic gradient descent to train model\n",
    "    # SGD \n",
    "    optimize = tf.keras.optimizers.SGD(learning_rate = 0.002)\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimize)\n",
    "    # save model weights validation loss improves\n",
    "    weights = ModelCheckpoint(\"best_weights.h5\", monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    # adjust learning rate when needed\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # one epoch completes when the entire training dataset is processed once by the model\n",
    "    model.fit(X_train, Y_train, epochs=12, batch_size=1, verbose=1, shuffle=False, validation_data=(X_test, Y_test), callbacks=[reduce_lr, weights])\n",
    "    actual = scaler.inverse_transform(Y_test)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    actual = np.squeeze(actual, axis=1)\n",
    "    predictions = np.squeeze(predictions, axis=1)\n",
    "\n",
    "    #reassign index before it goes into model\n",
    "    test_df = pd.DataFrame({'Actual': actual, 'Predicted': predictions.flatten()})\n",
    "    print(test_df)\n",
    "    #return model, loss, predict, predict_inv, Y_test, scaler\n",
    "\n",
    "    '''\n",
    "    # Plotting test set\n",
    "    graph.plot(df.index[-train_test_slice:], predictions, label=\"Predicted\")\n",
    "    graph.plot(df.index[-train_test_slice:], actual, label=\"Actual\")\n",
    "    graph.xlabel('Date')\n",
    "    graph.ylabel('Stock Price')\n",
    "    graph.legend()\n",
    "    graph.savefig('predicted_stock_prices_lstm3_test.png')\n",
    "    graph.show()\n",
    "    '''\n",
    "    return model, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(model, df, future_date, scaler, feature_length=20):\n",
    "    # iterate through today's date until future date\n",
    "    for i in range((datetime.strptime(future_date, '%Y-%m-%d') - df.index[-1]).days):\n",
    "        # specify close values\n",
    "        feature_column = df['Close'].values\n",
    "        # pick out last 20 days\n",
    "        time_steps = feature_column[-feature_length:]\n",
    "        # reshape array\n",
    "        time_steps = time_steps.reshape(feature_length, 1)\n",
    "        # scale array\n",
    "        time_steps = scaler.transform(time_steps)\n",
    "        prediction = model.predict(time_steps.reshape(1, feature_length, 1))\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "        # concatenate results with og dataframe\n",
    "        df_forecast = pd.DataFrame(prediction, index=[df.index[-1] + timedelta(days=1)], columns=['Close'])\n",
    "        df = pd.concat([df, df_forecast])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SARIMA\n",
    "*why?*  \n",
    "\n",
    "SARIMA MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import libraries for SARIMA model\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal - fit stepwise auto-ARIMA\n",
    "#!pip install pmdarima\n",
    "\n",
    "# Remove any duplicate index\n",
    "apple_data = apple_data.loc[~apple_data.index.duplicated(keep='first')]\n",
    "\n",
    "#Filter only required data\n",
    "\n",
    "apple_data = apple_data[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the APPL data into a standard range using MinMaxScaler ()\n",
    "Feature_Scaler = MinMaxScaler()\n",
    "\n",
    "#Transform current APPL data\n",
    "apple_transformed = pd.DataFrame(np.squeeze(Feature_Scaler.fit_transform(apple_data), axis=1), columns=[\"Close\"], index=apple_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarima_model = pm.auto_arima(apple_transformed[\"Close\"], start_p=1, start_q=1,\n",
    "                         test='adf',\n",
    "                         max_p=3, max_q=3,\n",
    "                         m=12, #12 is the frequency of the cycle (yearly)\n",
    "                         start_P=0,\n",
    "                         seasonal=True, #set to seasonal\n",
    "                         d=None,\n",
    "                         D=1, #order of the seasonal differencing\n",
    "                         trace=False,\n",
    "                         error_action='ignore',\n",
    "                         suppress_warnings=True,\n",
    "                         stepwise=True)\n",
    "\n",
    "# start_p=1, start_q=1: Sets the initial values for the order of the AR (AutoRegressive) and MA (Moving Average) components in the non-seasonal part of the model.\n",
    "# test='adf': Specifies the use of the Augmented Dickey-Fuller (ADF) test to determine whether the time series is stationary and to help in determining the need for differencing (`d` parameter).\n",
    "# max_p=3, max_q=3: Specifies the maximum values for the `p` and `q` parameters to consider during the model fitting process.\n",
    "# start_P=0: Sets the initial value for the order of the seasonal AR component.\n",
    "# d=None: The order of non-seasonal differencing is not specified, which allows the function to determine it automatically.\n",
    "# trace=False: This means that the function will not print out diagnostic information about the steps it's taking.\n",
    "# error_action='ignore': Instructs the function to ignore errors and try different combinations of parameters.\n",
    "# suppress_warnings=True: Suppresses convergence warnings, which can be frequent in ARIMA modeling.\n",
    "# stepwise=True: Enables a stepwise search to efficiently find the best model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA model plot diagnostics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize with Pickle and save it as pkl\n",
    "with open('sarima_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(sarima_model, pkl)\n",
    "\n",
    "# Desiarilize the content of the file back into a Python object\n",
    "with open('sarima_model.pkl', 'rb') as pkl:\n",
    "    loaded_model = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=forecast(loaded_model, apple_transformed, \"2023-12-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(test[\"Actual\"][-120:], color='#1f76b4')\n",
    "plt.plot(test[\"Prediction\"], color='darkgreen')\n",
    "plt.fill_between(test.index,\n",
    "                test[\"Low\"],\n",
    "                test[\"High\"],\n",
    "                color='k', alpha=.15)\n",
    "\n",
    "plt.title(\"SARIMA - Forecast of APPL Stock Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LSTM\n",
    "*why?*  \n",
    "\n",
    "LSTM MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple_data = yf.download(\"AAPL\", start = \"2020-01-01\", interval = '1d')\n",
    "apple_df_test = apple_data.reset_index()\n",
    "apple_df = apple_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_transformed = pd.DataFrame(\n",
    "    np.squeeze(\n",
    "        scaler.fit_transform(\n",
    "            apple_df[[\"Close\"]])), columns=[\"Close\"], index=apple_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_vals, target_vals = features_targets(data_transformed[\"Close\"].values, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals_cutoff = apple_df.loc[apple_df['Date'] >= '2022-01-01']\n",
    "slice = train_vals_cutoff.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X_train, X_test, Y_train, Y_test = create_model(\n",
    "        time_step_vals, target_vals, apple_df, data_transformed[\"Close\"].values, slice, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x = np.concatenate((X_train, X_test), axis = 0)\n",
    "total_y = np.concatenate((Y_train, Y_test), axis = 0)\n",
    "final_predict = model.predict(total_x)\n",
    "final_predict = scaler.inverse_transform(final_predict)\n",
    "actual = scaler.inverse_transform(total_y)\n",
    "final_predict = np.squeeze(final_predict, axis = 1)\n",
    "actual = np.squeeze(actual, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot(final_predict, label = \"total predict\")\n",
    "graph.plot(actual, label = \"total actual\")\n",
    "graph.xlabel('Date')\n",
    "graph.ylabel('Stock Price')\n",
    "graph.legend()\n",
    "graph.savefig('test.png')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_transformed[-21:-1].values.reshape(1,20,1))\n",
    "data = pd.DataFrame(apple_data)\n",
    "test = predict_lstm(model, data, '2024-02-01', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: is this where the combo assessment goes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Why does SARIMA only have 1 function, but LSTM has several?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: do you want to output plots as pngs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify google cloud project information\n",
    "dataset_id = 'predicted_prices'\n",
    "table_id = 'SARIMA and LTSM Predicted Prices'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "# enter combined data load_data = client.load_table_from_dataframe(combined data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "** a bit more detail about why a client would need this and what value it would bring to their work **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
