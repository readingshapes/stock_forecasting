{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yahoo finance data\n",
    "import yfinance as yf\n",
    "# import stockstats data\n",
    "from stockstats import StockDataFrame as ss\n",
    "\n",
    "# import necessary libraries\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "import pydoc\n",
    "import os\n",
    "import pyarrow\n",
    "import pandas_gbq\n",
    "import statsmodels\n",
    "import tensorflow\n",
    "\n",
    "#import libraries for SARIMA model\n",
    "import pmdarima as pm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "                  open        high         low       close   adj close  \\\n",
      "Date                                                                     \n",
      "2020-01-02   74.059998   75.150002   73.797501   75.087502   73.152657   \n",
      "2020-01-03   74.287498   75.144997   74.125000   74.357498   72.441444   \n",
      "2020-01-06   73.447502   74.989998   73.187500   74.949997   73.018700   \n",
      "2020-01-07   74.959999   75.224998   74.370003   74.597504   72.675278   \n",
      "2020-01-08   74.290001   76.110001   74.290001   75.797501   73.844345   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2024-01-24  195.419998  196.380005  194.339996  194.500000  194.500000   \n",
      "2024-01-25  195.220001  196.270004  193.110001  194.169998  194.169998   \n",
      "2024-01-26  194.270004  194.759995  191.940002  192.419998  192.419998   \n",
      "2024-01-29  192.009995  192.199997  189.580002  191.729996  191.729996   \n",
      "2024-01-30  190.940002  191.800003  187.470001  188.039993  188.039993   \n",
      "\n",
      "               volume    stochrsi      macd       mfi  \n",
      "Date                                                   \n",
      "2020-01-02  135480400         NaN  0.000000  0.500000  \n",
      "2020-01-03  146322800         NaN -0.016378  0.500000  \n",
      "2020-01-06  118387200  100.000000 -0.002496  0.500000  \n",
      "2020-01-07  108872000   76.992988 -0.008847  0.500000  \n",
      "2020-01-08  132079200  100.000000  0.035638  0.500000  \n",
      "...               ...         ...       ...       ...  \n",
      "2024-01-24   53631300   94.974494  0.590807  0.632481  \n",
      "2024-01-25   54822100   92.455043  0.876741  0.643113  \n",
      "2024-01-26   44553400   72.999542  0.951170  0.655327  \n",
      "2024-01-29   47145600   66.461205  0.943601  0.586432  \n",
      "2024-01-30   55186221   35.400724  0.632559  0.520693  \n",
      "\n",
      "[1026 rows x 9 columns]\n",
      "<bound method TickerBase.get_capital_gains of yfinance.Ticker object <AAPL>>\n"
     ]
    }
   ],
   "source": [
    "apple_ticker = yf.Ticker(\"AAPL\")\n",
    "apple_data = yf.download(\"AAPL\", start = '2020-01-01', interval = '1d')\n",
    "apple_df = ss.retype(apple_data)\n",
    "\n",
    "apple_data[['stochrsi', 'macd', 'mfi']] = apple_df[['stochrsi', 'macd', 'mfi']]\n",
    "print(apple_data)\n",
    "print(apple_ticker.get_capital_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=black-vehicle-406619, location=US, id=3ec432d6-2982-4390-b1e8-5704b301ee89>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "# import google cloud service account and bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# specify google cloud project information\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'black-vehicle-406619-bf2e31773163.json')\n",
    "project_id = 'black-vehicle-406619'\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "dataset_id = 'stocks_ds'\n",
    "table_id = '20yrs_stockdata'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "load_data = client.load_table_from_dataframe(apple_data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal - fit stepwise auto-ARIMA\n",
    "#!pip install pmdarima\n",
    "\n",
    "# Remove any duplicate index\n",
    "apple_data = apple_data.loc[~apple_data.index.duplicated(keep='first')]\n",
    "\n",
    "#Filter only required data\n",
    "\n",
    "apple_data = apple_data[['close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the APPL data into a standard range using MinMaxScaler ()\n",
    "\n",
    "Feature_Scaler = MinMaxScaler()\n",
    "\n",
    "#Transform current APPL data\n",
    "\n",
    "apple_transformed = pd.DataFrame(np.squeeze(Feature_Scaler.fit_transform(apple_data), axis=1), columns=[\"Close\"], index=apple_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarima_model = pm.auto_arima(apple_transformed[\"Close\"], start_p=1, start_q=1,\n",
    "                         test='adf',\n",
    "                         max_p=3, max_q=3,\n",
    "                         m=12, #12 is the frequency of the cycle (yearly)\n",
    "                         start_P=0,\n",
    "                         seasonal=True, #set to seasonal\n",
    "                         d=None,\n",
    "                         D=1, #order of the seasonal differencing\n",
    "                         trace=False,\n",
    "                         error_action='ignore',\n",
    "                         suppress_warnings=True,\n",
    "                         stepwise=True)\n",
    "\n",
    "# start_p=1, start_q=1: Sets the initial values for the order of the AR (AutoRegressive) and MA (Moving Average) components in the non-seasonal part of the model.\n",
    "# test='adf': Specifies the use of the Augmented Dickey-Fuller (ADF) test to determine whether the time series is stationary and to help in determining the need for differencing (`d` parameter).\n",
    "# max_p=3, max_q=3: Specifies the maximum values for the `p` and `q` parameters to consider during the model fitting process.\n",
    "# start_P=0: Sets the initial value for the order of the seasonal AR component.\n",
    "# d=None: The order of non-seasonal differencing is not specified, which allows the function to determine it automatically.\n",
    "# trace=False: This means that the function will not print out diagnostic information about the steps it's taking.\n",
    "# error_action='ignore': Instructs the function to ignore errors and try different combinations of parameters.\n",
    "# suppress_warnings=True: Suppresses convergence warnings, which can be frequent in ARIMA modeling.\n",
    "# stepwise=True: Enables a stepwise search to efficiently find the best model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sarima_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wjulia800/stock_forecasting/combo.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sarima_model\u001b[39m.\u001b[39mplot_diagnostics(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m,\u001b[39m12\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sarima_model' is not defined"
     ]
    }
   ],
   "source": [
    "sarima_model.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sarima_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wjulia800/stock_forecasting/combo.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Serialize with Pickle and save it as pkl\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msarima_model.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m pkl:\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(sarima_model, pkl)\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Desiarilize the content of the file back into a Python object\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://970-cs-127461888618-default.cs-us-west1-ijlt.cloudshell.dev/home/wjulia800/stock_forecasting/combo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msarima_model.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m pkl:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sarima_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Serialize with Pickle and save it as pkl\n",
    "with open('sarima_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(sarima_model, pkl)\n",
    "\n",
    "# Desiarilize the content of the file back into a Python object\n",
    "with open('sarima_model.pkl', 'rb') as pkl:\n",
    "    loaded_model = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, df, forecast_date):\n",
    "    # Forecast\n",
    "    n_periods = (datetime.strptime(forecast_date, '%Y-%m-%d') - df.index[-1]).days\n",
    "    fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(days=1), periods = n_periods, freq='D')\n",
    "\n",
    "    # Make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted.values, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    #Concatenate the original DataFrame with forecasted values and confidence intervals\n",
    "    df_result = pd.concat([df, fitted_series, lower_series, upper_series], axis=1)\n",
    "    df_result.columns = [\"Actual\", \"Prediction\", \"Low\", \"High\"]\n",
    "\n",
    "    #Inverse transform the scaled values to their original scale\n",
    "    for column in df_result.columns:\n",
    "      df_result[column] = Feature_Scaler.inverse_transform(df_result[column].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjulia800/.local/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/wjulia800/.local/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "test=forecast(loaded_model, apple_transformed, '2024-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(test[\"Actual\"][-120:], color='#1f76b4')\n",
    "plt.plot(test[\"Prediction\"], color='darkgreen')\n",
    "plt.fill_between(test.index,\n",
    "                test[\"Low\"],\n",
    "                test[\"High\"],\n",
    "                color='k', alpha=.15)\n",
    "\n",
    "plt.title(\"SARIMA - Forecast of APPL Stock Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_data = yf.download(\"AAPL\", start = \"2020-01-01\", interval = '1d')\n",
    "apple_df_test = apple_data.reset_index()\n",
    "apple_df = apple_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_transformed = pd.DataFrame(\n",
    "    np.squeeze(\n",
    "        scaler.fit_transform(\n",
    "            apple_df[[\"Close\"]])), columns=[\"Close\"], index=apple_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_targets(data, feature_length):\n",
    "    # feature length is the number of time steps in the input sequence\n",
    "    # targets are the values the model is trying to forecast\n",
    "    time_step_list, close_label_list = [], []\n",
    "    \n",
    "    # iterate through (length of sequential data) to (length of seq data - feature length)\n",
    "    for i in range(len(data) - feature_length):\n",
    "        # this will get the vals leading up to the target\n",
    "        time_steps = data[i : i + feature_length]\n",
    "        time_step_list.append(time_steps)\n",
    "        # this will get the target val at this point\n",
    "        labels = data[i + feature_length]\n",
    "        close_label_list.append(labels)\n",
    "\n",
    "    # reshape lists to be suitable for network algo\n",
    "    time_step_list = np.array(time_step_list).reshape(len(time_step_list), feature_length, 1)\n",
    "    close_label_list = np.array(close_label_list).reshape(len(close_label_list), 1)\n",
    "\n",
    "    return time_step_list, close_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_vals, target_vals = features_targets(data_transformed[\"Close\"].values, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals_cutoff = apple_df.loc[apple_df['Date'] >= '2022-01-01']\n",
    "slice = train_vals_cutoff.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X, Y, df, data, train_test_slice, scaler):\n",
    "    # training set: set to train the machine learning model\n",
    "    # testing set: set used to test model after model has been trained\n",
    "    # train set is 70% of data, test set is the rest (30%)\n",
    "    X_train, X_test = X[:-train_test_slice], X[-train_test_slice:]\n",
    "    Y_train, Y_test = Y[:-train_test_slice], Y[-train_test_slice:]\n",
    "\n",
    "    # initialize empty model where nodes have input and output with Keras\n",
    "    model = Sequential()\n",
    "    # create a bidirectional LSTM: \n",
    "    # - 100 cells\n",
    "    # - return output for input\n",
    "    # - reduce overfitting w current dropout\n",
    "    # - specify # of steps for target\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.1, input_shape=(X_train.shape[1], 1))))\n",
    "    # provide additional processing with undirectional layer\n",
    "    model.add(LSTM(50, recurrent_dropout=0.1))\n",
    "    # add dropout and dense layers\n",
    "    # randomly sets 20% of inputs to 0 to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 25 output units\n",
    "    model.add(Dense(20, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # create a connected layer with 10 output units\n",
    "    model.add(Dense(10))\n",
    "    # create a connected layer with 1 output unit\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # optimize model using stochastic gradient descent to train model\n",
    "    # SGD \n",
    "    optimize = tf.keras.optimizers.SGD(learning_rate = 0.002)\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimize)\n",
    "    # save model weights validation loss improves\n",
    "    weights = ModelCheckpoint(\"best_weights.h5\", monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    # adjust learning rate when needed\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    # one epoch completes when the entire training dataset is processed once by the model\n",
    "    model.fit(X_train, Y_train, epochs=12, batch_size=1, verbose=1, shuffle=False, validation_data=(X_test, Y_test), callbacks=[reduce_lr, weights])\n",
    "    actual = scaler.inverse_transform(Y_test)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    actual = np.squeeze(actual, axis=1)\n",
    "    predictions = np.squeeze(predictions, axis=1)\n",
    "\n",
    "    #reassign index before it goes into model\n",
    "    test_df = pd.DataFrame({'Actual': actual, 'Predicted': predictions.flatten()})\n",
    "    \n",
    "    #print test_df plot\n",
    "    '''\n",
    "    # Plotting test set\n",
    "    graph.plot(df.index[-train_test_slice:], predictions, label=\"Predicted\")\n",
    "    graph.plot(df.index[-train_test_slice:], actual, label=\"Actual\")\n",
    "    graph.xlabel('Date')\n",
    "    graph.ylabel('Stock Price')\n",
    "    graph.legend()\n",
    "    graph.savefig('predicted_stock_prices_lstm3_test.png')\n",
    "    graph.show()\n",
    "    '''\n",
    "    return model, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign model, x training set, y training set, x testing set, and y testing set\n",
    "lstm_model, X_train, X_test, Y_train, Y_test = create_model(\n",
    "        time_step_vals, target_vals, apple_df, data_transformed[\"Close\"].values, slice, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate x sets, horiz axis\n",
    "total_x = np.concatenate((X_train, X_test), axis = 0)\n",
    "# concatenate y sets, horiz axis\n",
    "total_y = np.concatenate((Y_train, Y_test), axis = 0)\n",
    "# predict x axis with predict function\n",
    "final_predict = model.predict(total_x)\n",
    "# inverse transform x axis predictions\n",
    "final_predict = scaler.inverse_transform(final_predict)\n",
    "# inverse transform y predictions\n",
    "actual = scaler.inverse_transform(total_y)\n",
    "final_predict = np.squeeze(final_predict, axis = 1)\n",
    "actual = np.squeeze(actual, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot(final_predict, label = \"total predict\")\n",
    "graph.plot(actual, label = \"total actual\")\n",
    "graph.xlabel('Date')\n",
    "graph.ylabel('Stock Price')\n",
    "graph.legend()\n",
    "graph.savefig('test.png')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(model, df, future_date, scaler, feature_length=20):\n",
    "    # iterate through today's date until future date\n",
    "    for i in range((datetime.strptime(future_date, '%Y-%m-%d') - df.index[-1]).days):\n",
    "        # specify close values\n",
    "        feature_column = df['Close'].values\n",
    "        # pick out last 20 days\n",
    "        time_steps = feature_column[-feature_length:]\n",
    "        # reshape array\n",
    "        time_steps = time_steps.reshape(feature_length, 1)\n",
    "        # scale array\n",
    "        time_steps = scaler.transform(time_steps)\n",
    "        prediction = model.predict(time_steps.reshape(1, feature_length, 1))\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "        # concatenate results with og dataframe\n",
    "        predicted_vals = pd.DataFrame(prediction, index=[df.index[-1] + timedelta(days=1)], columns=['Close'])\n",
    "        df = pd.concat([df, predicted_vals])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape model's last twenty values into array of 1 row, 20 cols, 1 scalar (cause of chosen feature length)\n",
    "data_transformed = data_transformed[-21:-1].values.reshape(1,20,1)\n",
    "# predict model with predict function\n",
    "lstm_model.predict(data_transformed)\n",
    "# create new df out of apple_data\n",
    "#data = pd.DataFrame(apple_data)\n",
    "test = predict_lstm(lstm_model, apple_df, '2024-02-01', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model(data_transformed, lstm_model, sarima_model, forecast_date, scaler):\n",
    "\n",
    "  # create dataframes for high and low values\n",
    "  df_high = pd.DataFrame(columns=['High'])\n",
    "  df_low = pd.DataFrame(columns=['Low'])\n",
    "  #last_initial_date = data_transformed.index[-1]\n",
    "  # create variable referring to last day\n",
    "  last_day = data_transformed.index[-1]\n",
    "\n",
    "  # iterate through the number of days between last day and future date\n",
    "  for i in range(datetime.strptime(forecast_date, '%Y-%m-%d') - last_day.days):\n",
    "    # call prediction function for lstm\n",
    "    df_lstm_temp = predict_lstm(lstm_model, data_transformed, (last_day + timedelta(days=1)).strftime('%Y-%m-%d'), scaler)\n",
    "    # call forecast function for sarima\n",
    "    df_sarima_temp = forecast(sarima_model, data_transformed, (last_day + timedelta(days=i+1)).strftime('%Y-%m-%d'), scaler)\n",
    "\n",
    "    # combine 40% of lstm results and 60% of sarima results for closing vals on last day (day to predict)\n",
    "    combination = 0.4 * (scaler.transform(df_lstm_temp.iloc[-1]['Close'].reshape(1,-1))) + 0.6 * (scaler.transform(df_sarima_temp.iloc[-1]['Prediction'].reshape(1,-1)))\n",
    "\n",
    "    # create new dataframe of combo :D\n",
    "    forecast = pd.DataFrame(combination, index = [data_transformed.index[-1] + timedelta(days=1)], columns = ['Close'])\n",
    "    data_transformed = pd.concat([data_transformed, forecast])\n",
    "    # create new dataframe of low values\n",
    "    df_low = pd.concat([df_low, pd.DataFrame(df_sarima_temp.iloc[-1]['Low'],\n",
    "                                          index=[data_transformed.index[-1]+ timedelta(days=1)],\n",
    "                                          columns=['Low'])\n",
    "    ])\n",
    "    # create new dataframe of high values\n",
    "    df_high = pd.concat([df_high, pd.DataFrame(df_sarima_temp.iloc[-1]['High'],\n",
    "                                          index=[data_transformed.index[-1]+ timedelta(days=1)],\n",
    "                                          columns=['High'])\n",
    "    ])\n",
    "\n",
    "  # squeeze final results to one axis, and inverse transform\n",
    "  df_final = pd.DataFrame(np.squeeze(scaler.inverse_transform(data_transformed)),\n",
    "                        index=data_transformed.index, columns=['Close'])\n",
    "\n",
    "  # final actual df values are all days leading up to last day that has occurred\n",
    "  df_final_actual = df_final[:last_day + timedelta(days=1)]\n",
    "  # final predicted df values are all future days\n",
    "  df_final_prediction = df_final[last_day + timedelta(days=1):]\n",
    "\n",
    "  return df_final_actual, df_final_prediction, df_low,  df_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ~final~ combined results\n",
    "combined_actual, combined_prediction, combined_df_low, combined_df_high = combined_model(data_transformed, lstm_model, sarima_model, '2024-03-01', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify google cloud project information\n",
    "dataset_id = 'predicted_prices'\n",
    "table_id = 'SARIMA and LTSM Predicted Prices'\n",
    "table_path = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# specify load reqs\n",
    "load_info = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "# enter combined data load_data = client.load_table_from_dataframe(combined data, table_path, job_config=load_info)\n",
    "load_data.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
